{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Brid Identification\n",
    "Kaggle competetion https://www.kaggle.com/c/dog-breed-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first import the required librariees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from os.path import join\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from datagen import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define few paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define few paths\n",
    "data_dir = './data'\n",
    "train_data_dir = './data/train'\n",
    "test_data_dir = './data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labels file\n",
    "label_file = pandas.read_csv(join(data_dir, 'labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 id             breed\n",
      "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
      "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
      "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
      "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
      "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever\n"
     ]
    }
   ],
   "source": [
    "print(label_file.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize few inportant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the number of classes\n",
    "n_classes = len(label_file['breed'].unique())\n",
    "# Total number of samples present, train + val\n",
    "n_samples = len(label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create important dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a label to idx dictionary. Its basically string label to int label dictionary.\n",
    "label_to_idx = {}\n",
    "# dictionary to map int label to string label\n",
    "idx_to_label = {}\n",
    "unique_labels = label_file['breed'].unique()\n",
    "for i in range(0, n_classes):\n",
    "    label_to_idx[unique_labels[i]] = i\n",
    "    idx_to_label[i] = unique_labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sets\n",
    "val_percentage = 0.1\n",
    "train, val = train_test_split(label_file, test_size=val_percentage)\n",
    "\n",
    "# this dictionary lists all the ids in train and val set\n",
    "partition = {'train': train['id'].tolist(), 'val': val['id'].tolist()}\n",
    "# this is a id to label dictionary\n",
    "id_to_labels = {}\n",
    "labels_oh = {}  # id to one-hot label dictionary\n",
    "target_oh = []\n",
    "for i in range(0, n_samples):\n",
    "    l_id, lb = label_file.iloc[i]  # id, string label\n",
    "    id_to_labels[l_id] = lb\n",
    "    # add the integer value of labels in the dataframe itself, this will help to create one-hot represetation\n",
    "    labels_oh[l_id] = label_to_idx[lb]\n",
    "    target_oh.append(label_to_idx[lb])\n",
    "    \n",
    "target_oh = np.array([[1 if target_oh[i] == j else 0 for j in range(n_classes)] for i in range(len(target_oh))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0\n",
      "Processing 10\n",
      "Processing 20\n",
      "Processing 30\n",
      "Processing 40\n",
      "Processing 50\n",
      "Processing 60\n",
      "Processing 70\n",
      "Processing 80\n",
      "Processing 90\n",
      "Processing 100\n",
      "Processing 110\n",
      "Processing 120\n",
      "Processing 130\n",
      "Processing 140\n",
      "Processing 150\n",
      "Processing 160\n",
      "Processing 170\n",
      "Processing 180\n",
      "Processing 190\n",
      "Processing 200\n",
      "Processing 210\n",
      "Processing 220\n",
      "Processing 230\n",
      "Processing 240\n",
      "Processing 250\n",
      "Processing 260\n",
      "Processing 270\n",
      "Processing 280\n",
      "Processing 290\n",
      "Processing 300\n",
      "Processing 310\n",
      "Processing 320\n",
      "Processing 330\n",
      "Processing 340\n",
      "Processing 350\n",
      "Processing 360\n",
      "Processing 370\n",
      "Processing 380\n",
      "Processing 390\n",
      "Processing 400\n",
      "Processing 410\n",
      "Processing 420\n",
      "Processing 430\n",
      "Processing 440\n",
      "Processing 450\n",
      "Processing 460\n",
      "Processing 470\n",
      "Processing 480\n",
      "Processing 490\n",
      "Processing 500\n",
      "Processing 510\n",
      "Processing 520\n",
      "Processing 530\n",
      "Processing 540\n",
      "Processing 550\n",
      "Processing 560\n",
      "Processing 570\n",
      "Processing 580\n",
      "Processing 590\n",
      "Processing 600\n",
      "Processing 610\n",
      "Processing 620\n",
      "Processing 630\n",
      "Processing 640\n",
      "Processing 650\n",
      "Processing 660\n",
      "Processing 670\n",
      "Processing 680\n",
      "Processing 690\n",
      "Processing 700\n",
      "Processing 710\n",
      "Processing 720\n",
      "Processing 730\n",
      "Processing 740\n",
      "Processing 750\n",
      "Processing 760\n",
      "Processing 770\n",
      "Processing 780\n",
      "Processing 790\n",
      "Processing 800\n",
      "Processing 810\n",
      "Processing 820\n",
      "Processing 830\n",
      "Processing 840\n",
      "Processing 850\n",
      "Processing 860\n",
      "Processing 870\n",
      "Processing 880\n",
      "Processing 890\n",
      "Processing 900\n",
      "Processing 910\n",
      "Processing 920\n",
      "Processing 930\n",
      "Processing 940\n",
      "Processing 950\n",
      "Processing 960\n",
      "Processing 970\n",
      "Processing 980\n",
      "Processing 990\n",
      "Processing 1000\n",
      "Processing 1010\n",
      "Processing 1020\n",
      "Processing 1030\n",
      "Processing 1040\n",
      "Processing 1050\n",
      "Processing 1060\n",
      "Processing 1070\n",
      "Processing 1080\n",
      "Processing 1090\n",
      "Processing 1100\n",
      "Processing 1110\n",
      "Processing 1120\n",
      "Processing 1130\n",
      "Processing 1140\n",
      "Processing 1150\n",
      "Processing 1160\n",
      "Processing 1170\n",
      "Processing 1180\n",
      "Processing 1190\n",
      "Processing 1200\n",
      "Processing 1210\n",
      "Processing 1220\n",
      "Processing 1230\n",
      "Processing 1240\n",
      "Processing 1250\n",
      "Processing 1260\n",
      "Processing 1270\n",
      "Processing 1280\n",
      "Processing 1290\n",
      "Processing 1300\n",
      "Processing 1310\n",
      "Processing 1320\n",
      "Processing 1330\n",
      "Processing 1340\n",
      "Processing 1350\n",
      "Processing 1360\n",
      "Processing 1370\n",
      "Processing 1380\n",
      "Processing 1390\n",
      "Processing 1400\n",
      "Processing 1410\n",
      "Processing 1420\n",
      "Processing 1430\n",
      "Processing 1440\n",
      "Processing 1450\n",
      "Processing 1460\n",
      "Processing 1470\n",
      "Processing 1480\n",
      "Processing 1490\n",
      "Processing 1500\n",
      "Processing 1510\n",
      "Processing 1520\n",
      "Processing 1530\n",
      "Processing 1540\n",
      "Processing 1550\n",
      "Processing 1560\n",
      "Processing 1570\n",
      "Processing 1580\n",
      "Processing 1590\n",
      "Processing 1600\n",
      "Processing 1610\n",
      "Processing 1620\n",
      "Processing 1630\n",
      "Processing 1640\n",
      "Processing 1650\n",
      "Processing 1660\n",
      "Processing 1670\n",
      "Processing 1680\n",
      "Processing 1690\n",
      "Processing 1700\n",
      "Processing 1710\n",
      "Processing 1720\n",
      "Processing 1730\n",
      "Processing 1740\n",
      "Processing 1750\n",
      "Processing 1760\n",
      "Processing 1770\n",
      "Processing 1780\n",
      "Processing 1790\n",
      "Processing 1800\n",
      "Processing 1810\n",
      "Processing 1820\n",
      "Processing 1830\n",
      "Processing 1840\n",
      "Processing 1850\n",
      "Processing 1860\n",
      "Processing 1870\n",
      "Processing 1880\n",
      "Processing 1890\n",
      "Processing 1900\n",
      "Processing 1910\n",
      "Processing 1920\n",
      "Processing 1930\n",
      "Processing 1940\n",
      "Processing 1950\n",
      "Processing 1960\n",
      "Processing 1970\n",
      "Processing 1980\n",
      "Processing 1990\n",
      "Processing 2000\n",
      "Processing 2010\n",
      "Processing 2020\n",
      "Processing 2030\n",
      "Processing 2040\n",
      "Processing 2050\n",
      "Processing 2060\n",
      "Processing 2070\n",
      "Processing 2080\n",
      "Processing 2090\n",
      "Processing 2100\n",
      "Processing 2110\n",
      "Processing 2120\n",
      "Processing 2130\n",
      "Processing 2140\n",
      "Processing 2150\n",
      "Processing 2160\n",
      "Processing 2170\n",
      "Processing 2180\n",
      "Processing 2190\n",
      "Processing 2200\n",
      "Processing 2210\n",
      "Processing 2220\n",
      "Processing 2230\n",
      "Processing 2240\n",
      "Processing 2250\n",
      "Processing 2260\n",
      "Processing 2270\n",
      "Processing 2280\n",
      "Processing 2290\n",
      "Processing 2300\n",
      "Processing 2310\n",
      "Processing 2320\n",
      "Processing 2330\n",
      "Processing 2340\n",
      "Processing 2350\n",
      "Processing 2360\n",
      "Processing 2370\n",
      "Processing 2380\n",
      "Processing 2390\n",
      "Processing 2400\n",
      "Processing 2410\n",
      "Processing 2420\n",
      "Processing 2430\n",
      "Processing 2440\n",
      "Processing 2450\n",
      "Processing 2460\n",
      "Processing 2470\n",
      "Processing 2480\n",
      "Processing 2490\n",
      "Processing 2500\n",
      "Processing 2510\n",
      "Processing 2520\n",
      "Processing 2530\n",
      "Processing 2540\n",
      "Processing 2550\n",
      "Processing 2560\n",
      "Processing 2570\n",
      "Processing 2580\n",
      "Processing 2590\n",
      "Processing 2600\n",
      "Processing 2610\n",
      "Processing 2620\n",
      "Processing 2630\n",
      "Processing 2640\n",
      "Processing 2650\n",
      "Processing 2660\n",
      "Processing 2670\n",
      "Processing 2680\n",
      "Processing 2690\n",
      "Processing 2700\n",
      "Processing 2710\n",
      "Processing 2720\n",
      "Processing 2730\n",
      "Processing 2740\n",
      "Processing 2750\n",
      "Processing 2760\n",
      "Processing 2770\n",
      "Processing 2780\n",
      "Processing 2790\n",
      "Processing 2800\n",
      "Processing 2810\n",
      "Processing 2820\n",
      "Processing 2830\n",
      "Processing 2840\n",
      "Processing 2850\n",
      "Processing 2860\n",
      "Processing 2870\n",
      "Processing 2880\n",
      "Processing 2890\n",
      "Processing 2900\n",
      "Processing 2910\n",
      "Processing 2920\n",
      "Processing 2930\n",
      "Processing 2940\n",
      "Processing 2950\n",
      "Processing 2960\n",
      "Processing 2970\n",
      "Processing 2980\n",
      "Processing 2990\n",
      "Processing 3000\n",
      "Processing 3010\n",
      "Processing 3020\n",
      "Processing 3030\n",
      "Processing 3040\n",
      "Processing 3050\n",
      "Processing 3060\n",
      "Processing 3070\n",
      "Processing 3080\n",
      "Processing 3090\n",
      "Processing 3100\n",
      "Processing 3110\n",
      "Processing 3120\n",
      "Processing 3130\n",
      "Processing 3140\n",
      "Processing 3150\n",
      "Processing 3160\n",
      "Processing 3170\n",
      "Processing 3180\n",
      "Processing 3190\n",
      "Processing 3200\n",
      "Processing 3210\n",
      "Processing 3220\n",
      "Processing 3230\n",
      "Processing 3240\n",
      "Processing 3250\n",
      "Processing 3260\n",
      "Processing 3270\n",
      "Processing 3280\n",
      "Processing 3290\n",
      "Processing 3300\n",
      "Processing 3310\n",
      "Processing 3320\n",
      "Processing 3330\n",
      "Processing 3340\n",
      "Processing 3350\n",
      "Processing 3360\n",
      "Processing 3370\n",
      "Processing 3380\n",
      "Processing 3390\n",
      "Processing 3400\n",
      "Processing 3410\n",
      "Processing 3420\n",
      "Processing 3430\n",
      "Processing 3440\n",
      "Processing 3450\n",
      "Processing 3460\n",
      "Processing 3470\n",
      "Processing 3480\n",
      "Processing 3490\n",
      "Processing 3500\n",
      "Processing 3510\n",
      "Processing 3520\n",
      "Processing 3530\n",
      "Processing 3540\n",
      "Processing 3550\n",
      "Processing 3560\n",
      "Processing 3570\n",
      "Processing 3580\n",
      "Processing 3590\n",
      "Processing 3600\n",
      "Processing 3610\n",
      "Processing 3620\n",
      "Processing 3630\n",
      "Processing 3640\n",
      "Processing 3650\n",
      "Processing 3660\n",
      "Processing 3670\n",
      "Processing 3680\n",
      "Processing 3690\n",
      "Processing 3700\n",
      "Processing 3710\n",
      "Processing 3720\n",
      "Processing 3730\n",
      "Processing 3740\n",
      "Processing 3750\n",
      "Processing 3760\n",
      "Processing 3770\n",
      "Processing 3780\n",
      "Processing 3790\n",
      "Processing 3800\n",
      "Processing 3810\n",
      "Processing 3820\n",
      "Processing 3830\n",
      "Processing 3840\n",
      "Processing 3850\n",
      "Processing 3860\n",
      "Processing 3870\n",
      "Processing 3880\n",
      "Processing 3890\n",
      "Processing 3900\n",
      "Processing 3910\n",
      "Processing 3920\n",
      "Processing 3930\n",
      "Processing 3940\n",
      "Processing 3950\n",
      "Processing 3960\n",
      "Processing 3970\n",
      "Processing 3980\n",
      "Processing 3990\n",
      "Processing 4000\n",
      "Processing 4010\n",
      "Processing 4020\n",
      "Processing 4030\n",
      "Processing 4040\n",
      "Processing 4050\n",
      "Processing 4060\n",
      "Processing 4070\n",
      "Processing 4080\n",
      "Processing 4090\n",
      "Processing 4100\n",
      "Processing 4110\n",
      "Processing 4120\n",
      "Processing 4130\n",
      "Processing 4140\n",
      "Processing 4150\n",
      "Processing 4160\n",
      "Processing 4170\n",
      "Processing 4180\n",
      "Processing 4190\n",
      "Processing 4200\n",
      "Processing 4210\n",
      "Processing 4220\n",
      "Processing 4230\n",
      "Processing 4240\n",
      "Processing 4250\n",
      "Processing 4260\n",
      "Processing 4270\n",
      "Processing 4280\n",
      "Processing 4290\n",
      "Processing 4300\n",
      "Processing 4310\n",
      "Processing 4320\n",
      "Processing 4330\n",
      "Processing 4340\n",
      "Processing 4350\n",
      "Processing 4360\n",
      "Processing 4370\n",
      "Processing 4380\n",
      "Processing 4390\n",
      "Processing 4400\n",
      "Processing 4410\n",
      "Processing 4420\n",
      "Processing 4430\n",
      "Processing 4440\n",
      "Processing 4450\n",
      "Processing 4460\n",
      "Processing 4470\n",
      "Processing 4480\n",
      "Processing 4490\n",
      "Processing 4500\n",
      "Processing 4510\n",
      "Processing 4520\n",
      "Processing 4530\n",
      "Processing 4540\n",
      "Processing 4550\n",
      "Processing 4560\n",
      "Processing 4570\n",
      "Processing 4580\n",
      "Processing 4590\n",
      "Processing 4600\n",
      "Processing 4610\n",
      "Processing 4620\n",
      "Processing 4630\n",
      "Processing 4640\n",
      "Processing 4650\n",
      "Processing 4660\n",
      "Processing 4670\n",
      "Processing 4680\n",
      "Processing 4690\n",
      "Processing 4700\n",
      "Processing 4710\n",
      "Processing 4720\n",
      "Processing 4730\n",
      "Processing 4740\n",
      "Processing 4750\n",
      "Processing 4760\n",
      "Processing 4770\n",
      "Processing 4780\n",
      "Processing 4790\n",
      "Processing 4800\n",
      "Processing 4810\n",
      "Processing 4820\n",
      "Processing 4830\n",
      "Processing 4840\n",
      "Processing 4850\n",
      "Processing 4860\n",
      "Processing 4870\n",
      "Processing 4880\n",
      "Processing 4890\n",
      "Processing 4900\n",
      "Processing 4910\n",
      "Processing 4920\n",
      "Processing 4930\n",
      "Processing 4940\n",
      "Processing 4950\n",
      "Processing 4960\n",
      "Processing 4970\n",
      "Processing 4980\n",
      "Processing 4990\n",
      "Processing 5000\n",
      "Processing 5010\n",
      "Processing 5020\n",
      "Processing 5030\n",
      "Processing 5040\n",
      "Processing 5050\n",
      "Processing 5060\n",
      "Processing 5070\n",
      "Processing 5080\n",
      "Processing 5090\n",
      "Processing 5100\n",
      "Processing 5110\n",
      "Processing 5120\n",
      "Processing 5130\n",
      "Processing 5140\n",
      "Processing 5150\n",
      "Processing 5160\n",
      "Processing 5170\n",
      "Processing 5180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5190\n",
      "Processing 5200\n",
      "Processing 5210\n",
      "Processing 5220\n",
      "Processing 5230\n",
      "Processing 5240\n",
      "Processing 5250\n",
      "Processing 5260\n",
      "Processing 5270\n",
      "Processing 5280\n",
      "Processing 5290\n",
      "Processing 5300\n",
      "Processing 5310\n",
      "Processing 5320\n",
      "Processing 5330\n",
      "Processing 5340\n",
      "Processing 5350\n",
      "Processing 5360\n",
      "Processing 5370\n",
      "Processing 5380\n",
      "Processing 5390\n",
      "Processing 5400\n",
      "Processing 5410\n",
      "Processing 5420\n",
      "Processing 5430\n",
      "Processing 5440\n",
      "Processing 5450\n",
      "Processing 5460\n",
      "Processing 5470\n",
      "Processing 5480\n",
      "Processing 5490\n",
      "Processing 5500\n",
      "Processing 5510\n",
      "Processing 5520\n",
      "Processing 5530\n",
      "Processing 5540\n",
      "Processing 5550\n",
      "Processing 5560\n",
      "Processing 5570\n",
      "Processing 5580\n",
      "Processing 5590\n",
      "Processing 5600\n",
      "Processing 5610\n",
      "Processing 5620\n",
      "Processing 5630\n",
      "Processing 5640\n",
      "Processing 5650\n",
      "Processing 5660\n",
      "Processing 5670\n",
      "Processing 5680\n",
      "Processing 5690\n",
      "Processing 5700\n",
      "Processing 5710\n",
      "Processing 5720\n",
      "Processing 5730\n",
      "Processing 5740\n",
      "Processing 5750\n",
      "Processing 5760\n",
      "Processing 5770\n",
      "Processing 5780\n",
      "Processing 5790\n",
      "Processing 5800\n",
      "Processing 5810\n",
      "Processing 5820\n",
      "Processing 5830\n",
      "Processing 5840\n",
      "Processing 5850\n",
      "Processing 5860\n",
      "Processing 5870\n",
      "Processing 5880\n",
      "Processing 5890\n",
      "Processing 5900\n",
      "Processing 5910\n",
      "Processing 5920\n",
      "Processing 5930\n",
      "Processing 5940\n",
      "Processing 5950\n",
      "Processing 5960\n",
      "Processing 5970\n",
      "Processing 5980\n",
      "Processing 5990\n",
      "Processing 6000\n",
      "Processing 6010\n",
      "Processing 6020\n",
      "Processing 6030\n",
      "Processing 6040\n",
      "Processing 6050\n",
      "Processing 6060\n",
      "Processing 6070\n",
      "Processing 6080\n",
      "Processing 6090\n",
      "Processing 6100\n",
      "Processing 6110\n",
      "Processing 6120\n",
      "Processing 6130\n",
      "Processing 6140\n",
      "Processing 6150\n",
      "Processing 6160\n",
      "Processing 6170\n",
      "Processing 6180\n",
      "Processing 6190\n",
      "Processing 6200\n",
      "Processing 6210\n",
      "Processing 6220\n",
      "Processing 6230\n",
      "Processing 6240\n",
      "Processing 6250\n",
      "Processing 6260\n",
      "Processing 6270\n",
      "Processing 6280\n",
      "Processing 6290\n",
      "Processing 6300\n",
      "Processing 6310\n",
      "Processing 6320\n",
      "Processing 6330\n",
      "Processing 6340\n",
      "Processing 6350\n",
      "Processing 6360\n",
      "Processing 6370\n",
      "Processing 6380\n",
      "Processing 6390\n",
      "Processing 6400\n",
      "Processing 6410\n",
      "Processing 6420\n",
      "Processing 6430\n",
      "Processing 6440\n",
      "Processing 6450\n",
      "Processing 6460\n",
      "Processing 6470\n",
      "Processing 6480\n",
      "Processing 6490\n",
      "Processing 6500\n",
      "Processing 6510\n",
      "Processing 6520\n",
      "Processing 6530\n",
      "Processing 6540\n",
      "Processing 6550\n",
      "Processing 6560\n",
      "Processing 6570\n",
      "Processing 6580\n",
      "Processing 6590\n",
      "Processing 6600\n",
      "Processing 6610\n",
      "Processing 6620\n",
      "Processing 6630\n",
      "Processing 6640\n",
      "Processing 6650\n",
      "Processing 6660\n",
      "Processing 6670\n",
      "Processing 6680\n",
      "Processing 6690\n",
      "Processing 6700\n",
      "Processing 6710\n",
      "Processing 6720\n",
      "Processing 6730\n",
      "Processing 6740\n",
      "Processing 6750\n",
      "Processing 6760\n",
      "Processing 6770\n",
      "Processing 6780\n",
      "Processing 6790\n",
      "Processing 6800\n",
      "Processing 6810\n",
      "Processing 6820\n",
      "Processing 6830\n",
      "Processing 6840\n",
      "Processing 6850\n",
      "Processing 6860\n",
      "Processing 6870\n",
      "Processing 6880\n",
      "Processing 6890\n",
      "Processing 6900\n",
      "Processing 6910\n",
      "Processing 6920\n",
      "Processing 6930\n",
      "Processing 6940\n",
      "Processing 6950\n",
      "Processing 6960\n",
      "Processing 6970\n",
      "Processing 6980\n",
      "Processing 6990\n",
      "Processing 7000\n",
      "Processing 7010\n",
      "Processing 7020\n",
      "Processing 7030\n",
      "Processing 7040\n",
      "Processing 7050\n",
      "Processing 7060\n",
      "Processing 7070\n",
      "Processing 7080\n",
      "Processing 7090\n",
      "Processing 7100\n",
      "Processing 7110\n",
      "Processing 7120\n",
      "Processing 7130\n",
      "Processing 7140\n",
      "Processing 7150\n",
      "Processing 7160\n",
      "Processing 7170\n",
      "Processing 7180\n",
      "Processing 7190\n",
      "Processing 7200\n",
      "Processing 7210\n",
      "Processing 7220\n",
      "Processing 7230\n",
      "Processing 7240\n",
      "Processing 7250\n",
      "Processing 7260\n",
      "Processing 7270\n",
      "Processing 7280\n",
      "Processing 7290\n",
      "Processing 7300\n",
      "Processing 7310\n",
      "Processing 7320\n",
      "Processing 7330\n",
      "Processing 7340\n",
      "Processing 7350\n",
      "Processing 7360\n",
      "Processing 7370\n",
      "Processing 7380\n",
      "Processing 7390\n",
      "Processing 7400\n",
      "Processing 7410\n",
      "Processing 7420\n",
      "Processing 7430\n",
      "Processing 7440\n",
      "Processing 7450\n",
      "Processing 7460\n",
      "Processing 7470\n",
      "Processing 7480\n",
      "Processing 7490\n",
      "Processing 7500\n",
      "Processing 7510\n",
      "Processing 7520\n",
      "Processing 7530\n",
      "Processing 7540\n",
      "Processing 7550\n",
      "Processing 7560\n",
      "Processing 7570\n",
      "Processing 7580\n",
      "Processing 7590\n",
      "Processing 7600\n",
      "Processing 7610\n",
      "Processing 7620\n",
      "Processing 7630\n",
      "Processing 7640\n",
      "Processing 7650\n",
      "Processing 7660\n",
      "Processing 7670\n",
      "Processing 7680\n",
      "Processing 7690\n",
      "Processing 7700\n",
      "Processing 7710\n",
      "Processing 7720\n",
      "Processing 7730\n",
      "Processing 7740\n",
      "Processing 7750\n",
      "Processing 7760\n",
      "Processing 7770\n",
      "Processing 7780\n",
      "Processing 7790\n",
      "Processing 7800\n",
      "Processing 7810\n",
      "Processing 7820\n",
      "Processing 7830\n",
      "Processing 7840\n",
      "Processing 7850\n",
      "Processing 7860\n",
      "Processing 7870\n",
      "Processing 7880\n",
      "Processing 7890\n",
      "Processing 7900\n",
      "Processing 7910\n",
      "Processing 7920\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "X = np.empty((n_samples, dim_x, dim_y, n_channels))\n",
    "y = np.empty((n_samples), dtype = int)\n",
    "# Generate data\n",
    "for i, ID in enumerate(partition['train']):\n",
    "    if i % 10 == 0:\n",
    "        print(\"Processing %s\" %  i)\n",
    "    # Store volume\n",
    "    img = cv2.imread('./data/train/'+ID + '.jpg')\n",
    "    X[i, :, :] = cv2.resize(img, (dim_y, dim_x))\n",
    "    X = X.astype('float32') / 255\n",
    "    y[i] = labels_oh[ID]\n",
    "y = np.array([[1 if y[i] == j else 0 for j in range(n_classes)] for i in range(y.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and initialize the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dim_x = 224\n",
    "dim_y = 224\n",
    "batch_size = 32\n",
    "n_channels = 3\n",
    "params = {'dim_x': dim_x,\n",
    "          'dim_y': dim_y,\n",
    "          'batch_size': 320,\n",
    "          'n_classes': n_classes,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## To use with ImageGenerator we must have some statistical measures about the data. The `get_sample()` function reads `n` samples to be used  by ImageGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sample(num, size):\n",
    "    sample = []\n",
    "    # print(partition['train'])\n",
    "    # print(partition['val'])\n",
    "    # print(labels_oh)\n",
    "    for i in range(0, num):\n",
    "        img_id = partition['train'][i]\n",
    "        # print(\"i=%s, id=%s\" % (i, img_id))\n",
    "        # print(partition['train'])\n",
    "        img = image.load_img(join(data_dir, 'train', '%s.jpg' % img_id), target_size=size)\n",
    "        img = image.img_to_array(img)\n",
    "        sample.append(img)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(dim_x, dim_y, n_channels))\n",
    "\n",
    "    '''\n",
    "    # #------------------------------ Model ---------------------------------------# #\n",
    "\n",
    "    # Design model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=9,\n",
    "                     strides=2,\n",
    "                     padding='valid',\n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.05),\n",
    "                     bias_initializer='zeros',\n",
    "                     kernel_regularizer=l2(0.001),\n",
    "                     bias_regularizer=None,\n",
    "                     input_shape=(dim_x, dim_y, n_channels)))\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(Dropout(rate=0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='valid',\n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.05),\n",
    "                     bias_initializer='zeros',\n",
    "                     kernel_regularizer=l2(0.001),\n",
    "                     bias_regularizer=None))\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=128,\n",
    "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.01),\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=l2(0.001),\n",
    "                    bias_regularizer=None))\n",
    "\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    '''\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=n_classes,\n",
    "              kernel_initializer=RandomNormal(mean=0.0, stddev=0.01),\n",
    "              bias_initializer='zeros',\n",
    "              kernel_regularizer=l2(0.001),\n",
    "              bias_regularizer=None)(x)\n",
    "\n",
    "    predictions = Activation('softmax')(x)\n",
    "\n",
    "    # create graph of new model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # freeze all convolutional base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model weights and compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "model = get_model()\n",
    "# define the checkpoint\n",
    "file_path = \"best_model.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(lr=0.01, decay=0.00016667),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(**params).generate(labels_oh, partition['train'], n_classes)\n",
    "validation_generator = DataGenerator(**params).generate(labels_oh, partition['val'], n_classes)\n",
    "\n",
    "# #------------------------------ Data Generator ---------------------------------------# #\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# let's say X_sample is a small-ish but statistically representative sample of your data\n",
    "X_sample = get_sample(5, (dim_x, dim_y))\n",
    "datagen.fit(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 32 32\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# let's say you have an ImageNet generator that yields ~10k samples at a time.\n",
    "'''\n",
    "nb_epoch = 5\n",
    "for e in range(nb_epoch):\n",
    "    print(\"epoch %d\" % e)\n",
    "    g = 1\n",
    "    for X_train, Y_train in DataGenerator(**params).generate(labels_oh, partition['train'], n_classes):  # these are chunks of ~10k pictures\n",
    "        print(\"<---------- Gen level = %s ----------->\" % g)\n",
    "        f = 1\n",
    "        for X_batch, Y_batch in datagen.flow(X_train, Y_train, batch_size=batch_size):  # these are chunks of 32 samples\n",
    "            print(\"Flow level = %s\" % f)\n",
    "            # print(\"X_batch = %s, Y_batch = %s\" % (X_batch.shape, Y_batch.shape))\n",
    "            loss = model.fit(X_batch, Y_batch,\n",
    "                             steps_per_epoch=int(len(X_train) / batch_size),\n",
    "                             callbacks=callbacks_list,\n",
    "                             verbose=2)\n",
    "            if f == int(len(X_train) / len(X_batch)):\n",
    "                break\n",
    "            f += 1\n",
    "        if g == int(len(partition['train']) / len(X_train)):\n",
    "            break\n",
    "        g += 1\n",
    "\n",
    "'''\n",
    "print(len(partition['train']), len(partition['val']), batch_size)\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    steps_per_epoch=len(partition['train']) / batch_size,\n",
    "                    validation_data=validation_generator,\n",
    "                    verbose=2,\n",
    "                    epochs=5,\n",
    "                    # validation_steps=2\n",
    "                    validation_steps=len(partition['val'])/batch_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Try some other way of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
