{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Brid Identification\n",
    "Kaggle competetion https://www.kaggle.com/c/dog-breed-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first import the required librariees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from os.path import join\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from datagen import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define few paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define few paths\n",
    "data_dir = './data'\n",
    "train_data_dir = './data/train'\n",
    "test_data_dir = './data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the labels file\n",
    "label_file = pandas.read_csv(join(data_dir, 'labels.csv'), nrows=3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 id             breed\n",
      "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
      "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
      "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
      "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
      "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever\n"
     ]
    }
   ],
   "source": [
    "print(label_file.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize few inportant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the number of classes\n",
    "n_classes = len(label_file['breed'].unique())\n",
    "# Total number of samples present, train + val\n",
    "n_samples = len(label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create important dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a label to idx dictionary. Its basically string label to int label dictionary.\n",
    "label_to_idx = {}\n",
    "# dictionary to map int label to string label\n",
    "idx_to_label = {}\n",
    "unique_labels = label_file['breed'].unique()\n",
    "for i in range(0, n_classes):\n",
    "    label_to_idx[unique_labels[i]] = i\n",
    "    idx_to_label[i] = unique_labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data sets\n",
    "val_percentage = 0.1\n",
    "train, val = train_test_split(label_file, test_size=val_percentage)\n",
    "\n",
    "# this dictionary lists all the ids in train and val set\n",
    "partition = {'train': train['id'].tolist(), 'val': val['id'].tolist()}\n",
    "# this is a id to label dictionary\n",
    "id_to_labels = {}\n",
    "labels_oh = {}  # id to one-hot label dictionary\n",
    "for i in range(0, n_samples):\n",
    "    l_id, lb = label_file.iloc[i]  # id, string label\n",
    "    id_to_labels[l_id] = lb\n",
    "    # add the integer value of labels in the dataframe itself, this will help to create one-hot represetation\n",
    "    labels_oh[l_id] = label_to_idx[lb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and initialize the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dim_x = 224\n",
    "dim_y = 224\n",
    "batch_size = 32\n",
    "n_channels = 3\n",
    "params = {'dim_x': dim_x,\n",
    "          'dim_y': dim_y,\n",
    "          'batch_size': 320,\n",
    "          'n_classes': n_classes,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use with ImageGenerator we must have some statistical measures about the data. The `get_sample()` function reads `n` samples to be used  by ImageGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample(num, size):\n",
    "    sample = []\n",
    "    # print(partition['train'])\n",
    "    # print(partition['val'])\n",
    "    # print(labels_oh)\n",
    "    for i in range(0, num):\n",
    "        img_id = partition['train'][i]\n",
    "        # print(\"i=%s, id=%s\" % (i, img_id))\n",
    "        # print(partition['train'])\n",
    "        img = image.load_img(join(data_dir, 'train', '%s.jpg' % img_id), target_size=size)\n",
    "        img = image.img_to_array(img)\n",
    "        sample.append(img)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(dim_x, dim_y, n_channels))\n",
    "\n",
    "    '''\n",
    "    # #------------------------------ Model ---------------------------------------# #\n",
    "\n",
    "    # Design model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=9,\n",
    "                     strides=2,\n",
    "                     padding='valid',\n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.05),\n",
    "                     bias_initializer='zeros',\n",
    "                     kernel_regularizer=l2(0.001),\n",
    "                     bias_regularizer=None,\n",
    "                     input_shape=(dim_x, dim_y, n_channels)))\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(Dropout(rate=0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='valid',\n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.05),\n",
    "                     bias_initializer='zeros',\n",
    "                     kernel_regularizer=l2(0.001),\n",
    "                     bias_regularizer=None))\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=128,\n",
    "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.01),\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=l2(0.001),\n",
    "                    bias_regularizer=None))\n",
    "\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    '''\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=n_classes,\n",
    "              kernel_initializer=RandomNormal(mean=0.0, stddev=0.01),\n",
    "              bias_initializer='zeros',\n",
    "              kernel_regularizer=l2(0.001),\n",
    "              bias_regularizer=None)(x)\n",
    "\n",
    "    predictions = Activation('softmax')(x)\n",
    "\n",
    "    # create graph of new model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # freeze all convolutional base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model weights and compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 33s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# get the model\n",
    "model = get_model()\n",
    "# define the checkpoint\n",
    "file_path = \"best_model.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(lr=0.01, decay=0.00016667),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(**params).generate(labels_oh, partition['train'], n_classes)\n",
    "validation_generator = DataGenerator(**params).generate(labels_oh, partition['val'], n_classes)\n",
    "\n",
    "# #------------------------------ Data Generator ---------------------------------------# #\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# let's say X_sample is a small-ish but statistically representative sample of your data\n",
    "X_sample = get_sample(5, (dim_x, dim_y))\n",
    "datagen.fit(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "<---------- Gen level = 1 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss improved from inf to 5.24723, saving model to best_model.h5\n",
      " - 9s - loss: 5.2472 - acc: 0.0375\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 12.5442 - acc: 0.0312\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 14.7197 - acc: 0.0187\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.0984 - acc: 0.0063\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2807 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2504 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2336 - acc: 0.0000e+00\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2226 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2141 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2068 - acc: 0.0000e+00\n",
      "<---------- Gen level = 2 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2003 - acc: 0.0000e+00\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6906 - acc: 0.0312\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1887 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6798 - acc: 0.0312\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1785 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1740 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6660 - acc: 0.0312\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1657 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1620 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6549 - acc: 0.0312\n",
      "<---------- Gen level = 3 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1554 - acc: 0.0000e+00\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.1451 - acc: 0.0625\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1497 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1472 - acc: 0.0000e+00\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6412 - acc: 0.0312\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1427 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6370 - acc: 0.0312\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1389 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1372 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1357 - acc: 0.0000e+00\n",
      "<---------- Gen level = 4 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1343 - acc: 0.0000e+00\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1329 - acc: 0.0000e+00\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.1244 - acc: 0.0625\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.3810 - acc: 0.0438\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1410 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1463 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1466 - acc: 0.0000e+00\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1448 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1424 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6363 - acc: 0.0312\n",
      "<---------- Gen level = 5 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.1305 - acc: 0.0625\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6322 - acc: 0.0312\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1341 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1326 - acc: 0.0000e+00\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1312 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1299 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6251 - acc: 0.0312\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1278 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1269 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6224 - acc: 0.0312\n",
      "<---------- Gen level = 6 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1254 - acc: 0.0000e+00\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6210 - acc: 0.0312\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1241 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1236 - acc: 0.0000e+00\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1231 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1227 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.7160 - acc: 0.0219\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1372 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6395 - acc: 0.0312\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1431 - acc: 0.0000e+00\n",
      "<---------- Gen level = 7 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6371 - acc: 0.0312\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6344 - acc: 0.0312\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1354 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6294 - acc: 0.0312\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1312 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1295 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1280 - acc: 0.0000e+00\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1268 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1258 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1248 - acc: 0.0000e+00\n",
      "<---------- Gen level = 8 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1241 - acc: 0.0000e+00\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1234 - acc: 0.0000e+00\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1228 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6185 - acc: 0.0312\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1218 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6177 - acc: 0.0312\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1210 - acc: 0.0000e+00\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1207 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1204 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1202 - acc: 0.0000e+00\n",
      "<---------- Gen level = 9 ----------->\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1200 - acc: 0.0000e+00\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 14.0925 - acc: 0.0125\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 14.8761 - acc: 0.0438\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 17.6386 - acc: 0.0000e+00\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 17.5795 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 17.2605 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2436 - acc: 0.0312\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.7743 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.6430 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.5478 - acc: 0.0000e+00\n",
      "epoch 1\n",
      "<---------- Gen level = 1 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.4759 - acc: 0.0000e+00\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.9169 - acc: 0.0312\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.3775 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.3434 - acc: 0.0000e+00\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.8122 - acc: 0.0312\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2933 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.7710 - acc: 0.0312\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2590 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2456 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2342 - acc: 0.0000e+00\n",
      "<---------- Gen level = 2 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.2170 - acc: 0.0625\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2158 - acc: 0.0000e+00\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2083 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2017 - acc: 0.0000e+00\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1958 - acc: 0.0000e+00\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1905 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1858 - acc: 0.0000e+00\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1816 - acc: 0.0000e+00\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1777 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6706 - acc: 0.0312\n",
      "<---------- Gen level = 3 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1711 - acc: 0.0000e+00\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.5863 - acc: 0.0312\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.2035 - acc: 0.0625\n",
      "Flow level = 4\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.7313 - acc: 0.0312\n",
      "Flow level = 5\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.7303 - acc: 0.0312\n",
      "Flow level = 6\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2234 - acc: 0.0000e+00\n",
      "Flow level = 7\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.2113 - acc: 0.0000e+00\n",
      "Flow level = 8\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6965 - acc: 0.0312\n",
      "Flow level = 9\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1908 - acc: 0.0000e+00\n",
      "Flow level = 10\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1829 - acc: 0.0000e+00\n",
      "<---------- Gen level = 4 ----------->\n",
      "Flow level = 1\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6726 - acc: 0.0312\n",
      "Flow level = 2\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 15.6671 - acc: 0.0312\n",
      "Flow level = 3\n",
      "Epoch 1/1\n",
      "Epoch 00001: loss did not improve\n",
      " - 2s - loss: 16.1660 - acc: 0.0000e+00\n",
      "Flow level = 4\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3d73f301698d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                              \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                              \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                              verbose=2)\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# let's say you have an ImageNet generator that yields ~10k samples at a time.\n",
    "nb_epoch = 5\n",
    "for e in range(nb_epoch):\n",
    "    print(\"epoch %d\" % e)\n",
    "    g = 1\n",
    "    for X_train, Y_train in DataGenerator(**params).generate(labels_oh, partition['train'], n_classes):  # these are chunks of ~10k pictures\n",
    "        print(\"<---------- Gen level = %s ----------->\" % g)\n",
    "        f = 1\n",
    "        for X_batch, Y_batch in datagen.flow(X_train, Y_train, batch_size=batch_size):  # these are chunks of 32 samples\n",
    "            print(\"Flow level = %s\" % f)\n",
    "            # print(\"X_batch = %s, Y_batch = %s\" % (X_batch.shape, Y_batch.shape))\n",
    "            loss = model.fit(X_batch, Y_batch,\n",
    "                             steps_per_epoch=int(len(X_train) / batch_size),\n",
    "                             callbacks=callbacks_list,\n",
    "                             verbose=2)\n",
    "            if f == int(len(X_train) / len(X_batch)):\n",
    "                break\n",
    "            f += 1\n",
    "        if g == int(len(partition['train']) / len(X_train)):\n",
    "            break\n",
    "        g += 1\n",
    "\n",
    "'''\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    steps_per_epoch=len(partition['train']) / batch_size,\n",
    "                    validation_data=validation_generator,\n",
    "                    verbose=2,\n",
    "                    epochs=5,\n",
    "                    # validation_steps=2\n",
    "                    validation_steps=len(partition['val'])/batch_size\n",
    "                    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
