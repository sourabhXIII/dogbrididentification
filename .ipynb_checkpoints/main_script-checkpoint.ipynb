{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Brid Identification\n",
    "Kaggle competetion https://www.kaggle.com/c/dog-breed-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first import the required librariees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from os.path import join\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from datagen import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define few paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define few paths\n",
    "data_dir = './data'\n",
    "train_data_dir = './data/train'\n",
    "test_data_dir = './data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labels file\n",
    "label_file = pandas.read_csv(join(data_dir, 'labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 id             breed\n",
      "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
      "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
      "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
      "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
      "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever\n"
     ]
    }
   ],
   "source": [
    "print(label_file.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize few inportant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the number of classes\n",
    "n_classes = len(label_file['breed'].unique())\n",
    "# Total number of samples present, train + val\n",
    "n_samples = len(label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create important dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a label to idx dictionary. Its basically string label to int label dictionary.\n",
    "label_to_idx = {}\n",
    "# dictionary to map int label to string label\n",
    "idx_to_label = {}\n",
    "unique_labels = label_file['breed'].unique()\n",
    "for i in range(0, n_classes):\n",
    "    label_to_idx[unique_labels[i]] = i\n",
    "    idx_to_label[i] = unique_labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sets\n",
    "val_percentage = 0.1\n",
    "train, val = train_test_split(label_file, test_size=val_percentage)\n",
    "\n",
    "# this dictionary lists all the ids in train and val set\n",
    "partition = {'train': train['id'].tolist(), 'val': val['id'].tolist()}\n",
    "# this is a id to label dictionary\n",
    "id_to_labels = {}\n",
    "labels_oh = {}  # id to one-hot label dictionary\n",
    "target_oh = []\n",
    "for i in range(0, n_samples):\n",
    "    l_id, lb = label_file.iloc[i]  # id, string label\n",
    "    id_to_labels[l_id] = lb\n",
    "    # add the integer value of labels in the dataframe itself, this will help to create one-hot represetation\n",
    "    labels_oh[l_id] = label_to_idx[lb]\n",
    "    target_oh.append(label_to_idx[lb])\n",
    "    \n",
    "target_oh = np.array([[1 if target_oh[i] == j else 0 for j in range(n_classes)] for i in range(len(target_oh))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0\n",
      "Processing 10\n",
      "Processing 20\n",
      "Processing 30\n",
      "Processing 40\n",
      "Processing 50\n",
      "Processing 60\n",
      "Processing 70\n",
      "Processing 80\n",
      "Processing 90\n",
      "Processing 100\n",
      "Processing 110\n",
      "Processing 120\n",
      "Processing 130\n",
      "Processing 140\n",
      "Processing 150\n",
      "Processing 160\n",
      "Processing 170\n",
      "Processing 180\n",
      "Processing 190\n",
      "Processing 200\n",
      "Processing 210\n",
      "Processing 220\n",
      "Processing 230\n",
      "Processing 240\n",
      "Processing 250\n",
      "Processing 260\n",
      "Processing 270\n",
      "Processing 280\n",
      "Processing 290\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "X = np.empty((n_samples, dim_x, dim_y, n_channels))\n",
    "y = np.empty((n_samples), dtype = int)\n",
    "# Generate data\n",
    "for i, ID in enumerate(partition['train']):\n",
    "    if i % 10 == 0:\n",
    "        print(\"Processing %s\" %  i)\n",
    "    # Store volume\n",
    "    img = cv2.imread('./data/train/'+ID + '.jpg')\n",
    "    X[i, :, :] = cv2.resize(img, (dim_y, dim_x))\n",
    "    X = X.astype('float32') / 255\n",
    "    y[i] = labels_oh[ID]\n",
    "y = np.array([[1 if y[i] == j else 0 for j in range(n_classes)] for i in range(y.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and initialize the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dim_x = 224\n",
    "dim_y = 224\n",
    "batch_size = 32\n",
    "n_channels = 3\n",
    "params = {'dim_x': dim_x,\n",
    "          'dim_y': dim_y,\n",
    "          'batch_size': 320,\n",
    "          'n_classes': n_classes,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## To use with ImageGenerator we must have some statistical measures about the data. The `get_sample()` function reads `n` samples to be used  by ImageGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sample(num, size):\n",
    "    sample = []\n",
    "    # print(partition['train'])\n",
    "    # print(partition['val'])\n",
    "    # print(labels_oh)\n",
    "    for i in range(0, num):\n",
    "        img_id = partition['train'][i]\n",
    "        # print(\"i=%s, id=%s\" % (i, img_id))\n",
    "        # print(partition['train'])\n",
    "        img = image.load_img(join(data_dir, 'train', '%s.jpg' % img_id), target_size=size)\n",
    "        img = image.img_to_array(img)\n",
    "        sample.append(img)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(dim_x, dim_y, n_channels))\n",
    "\n",
    "    '''\n",
    "    # #------------------------------ Model ---------------------------------------# #\n",
    "\n",
    "    # Design model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=9,\n",
    "                     strides=2,\n",
    "                     padding='valid',\n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.05),\n",
    "                     bias_initializer='zeros',\n",
    "                     kernel_regularizer=l2(0.001),\n",
    "                     bias_regularizer=None,\n",
    "                     input_shape=(dim_x, dim_y, n_channels)))\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(Dropout(rate=0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='valid',\n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.05),\n",
    "                     bias_initializer='zeros',\n",
    "                     kernel_regularizer=l2(0.001),\n",
    "                     bias_regularizer=None))\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=128,\n",
    "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.01),\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=l2(0.001),\n",
    "                    bias_regularizer=None))\n",
    "\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    '''\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=n_classes,\n",
    "              kernel_initializer=RandomNormal(mean=0.0, stddev=0.01),\n",
    "              bias_initializer='zeros',\n",
    "              kernel_regularizer=l2(0.001),\n",
    "              bias_regularizer=None)(x)\n",
    "\n",
    "    predictions = Activation('softmax')(x)\n",
    "\n",
    "    # create graph of new model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # freeze all convolutional base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model weights and compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "model = get_model()\n",
    "# define the checkpoint\n",
    "file_path = \"best_model.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(lr=0.01, decay=0.00016667),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(**params).generate(labels_oh, partition['train'], n_classes)\n",
    "validation_generator = DataGenerator(**params).generate(labels_oh, partition['val'], n_classes)\n",
    "\n",
    "# #------------------------------ Data Generator ---------------------------------------# #\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# let's say X_sample is a small-ish but statistically representative sample of your data\n",
    "X_sample = get_sample(5, (dim_x, dim_y))\n",
    "datagen.fit(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 32 32\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# let's say you have an ImageNet generator that yields ~10k samples at a time.\n",
    "'''\n",
    "nb_epoch = 5\n",
    "for e in range(nb_epoch):\n",
    "    print(\"epoch %d\" % e)\n",
    "    g = 1\n",
    "    for X_train, Y_train in DataGenerator(**params).generate(labels_oh, partition['train'], n_classes):  # these are chunks of ~10k pictures\n",
    "        print(\"<---------- Gen level = %s ----------->\" % g)\n",
    "        f = 1\n",
    "        for X_batch, Y_batch in datagen.flow(X_train, Y_train, batch_size=batch_size):  # these are chunks of 32 samples\n",
    "            print(\"Flow level = %s\" % f)\n",
    "            # print(\"X_batch = %s, Y_batch = %s\" % (X_batch.shape, Y_batch.shape))\n",
    "            loss = model.fit(X_batch, Y_batch,\n",
    "                             steps_per_epoch=int(len(X_train) / batch_size),\n",
    "                             callbacks=callbacks_list,\n",
    "                             verbose=2)\n",
    "            if f == int(len(X_train) / len(X_batch)):\n",
    "                break\n",
    "            f += 1\n",
    "        if g == int(len(partition['train']) / len(X_train)):\n",
    "            break\n",
    "        g += 1\n",
    "\n",
    "'''\n",
    "print(len(partition['train']), len(partition['val']), batch_size)\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    steps_per_epoch=len(partition['train']) / batch_size,\n",
    "                    validation_data=validation_generator,\n",
    "                    verbose=2,\n",
    "                    epochs=5,\n",
    "                    # validation_steps=2\n",
    "                    validation_steps=len(partition['val'])/batch_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Try some other way of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
